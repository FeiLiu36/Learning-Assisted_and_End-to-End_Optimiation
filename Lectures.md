# Lectures

+ [2020HUAWEI_Learning to optimize研讨会]

  链接：https://pan.baidu.com/s/1b9bbJiLOhlT2y2vBQM6i-A 
  提取码：3p6v

  + 基本信息：2020.12.18

  + 摘要：

  + 笔记：

    张老师的观点：

    1。 启发式算法发展多年，虽理论薄弱，但是形成了一些合理且行之有效的算法设计原则，这些原则虽大多无十分严格的理论支撑，但容易理解可以解释。一些启发性算法换个角度也能解释成梯度法。现代一些learning to optimize 承deep learning 之威力，但有黑箱操作的感觉。如果能解决这个问题，对learning to optimize 的应用推广有力。如何增加解释性？

    2。一门学科成熟，大至要有基本概念、基本原理、基本方法 及应用。20年后大学对研究生能不能开一门learning to optimize 的课？这门课应该讲什么？我觉的对比其它学科可能会有启发，至少太复杂讲不清的东西不值得放到教材里。我觉的从这个角度展望未来对目前科研工作安排有利。我十分想向各位老师请教这一问题。

    3。learning to optimize 从数据中学习算法。经典算法设计用经验用问题先验知识设计算法，我个人觉得两者结合有很多可能工作，可能更有实用价值，比如算法参数配置问题可以model 成机器学习问题。理解两种方法的长短，对思考进一步工作有益。什么是这两个方法的本质优劣之处？如何互补？

    4。从复杂度理论看，概率意义下复杂性分析有可能对理解learn to optimize 有益。80年代有概率意义下的simplex method 算法分析，这些技术有无可能分析learn to optimize方法？需要从什么角度思考入手？

    5。 应用方面，周知问题建模重要，现在ai在建模方面能不能有些作为？

    万丈高楼平地起，我觉的不忘传统方法之优势，思考什么是可以做的，什么是不太可能的事情，思考如何利用当今ai 之成果推动启发性方法发展，铁树也许会开花。

